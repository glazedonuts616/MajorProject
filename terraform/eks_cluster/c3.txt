provider "aws" {
  region = "us-east-1"
}



module "vpc" {
  source = "terraform-aws-modules/vpc/aws"


  name = "my-cluster-vpc"
  cidr = "10.10.0.0/16"


  azs             = ["us-east-1a", "us-east-1b"]
  private_subnets = ["10.10.1.0/24", "10.10.2.0/24"]
  public_subnets  = ["10.10.101.0/24", "10.10.102.0/24"]


  enable_nat_gateway = true
  single_nat_gateway = true


  enable_vpn_gateway = true




  tags = {
    Terraform = "true"
    Environment = "dev"
  }
}



# EKS Module
module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"
  cluster_name    = "my-terra-cluster"
  cluster_version = "1.31"
  subnet_ids         = module.vpc.private_subnets
  control_plane_subnet_ids = module.vpc.public_subnets
  vpc_id          = module.vpc.vpc_id
  node_iam_role_name = "Admin"
  node_iam_role_permissions_boundary = "arn:aws:iam::aws:policy/AdministratorAccess" 
  cluster_endpoint_public_access = true
  cluster_endpoint_private_access = true


  eks_managed_node_groups  = {
    terra-eks = {
        cluster_name = module.eks.cluster_name
        cluster_version = module.eks.cluster_version
        cluster_primary_security_group_id = module.eks.cluster_primary_security_group_id
        vpc_security_group_ids =[aws_security_group.eks_nodes_sg.id]
        min_size = 2
        max_size = 3
        desired_size = 2
        ami_id        = "ami-0453ec754f44f9a4a"
        instance_types = ["t3.small"]
        key_name    = "My-first-key"
        additional_security_group_ids = [aws_security_group.eks_nodes_sg.id]  # Attach new node SG
        create_node_security_group = false
        node_iam_role_name= module.eks.node_iam_role_name
        node_iam_role_permissions_boundary = "arn:aws:iam::aws:policy/AdministratorAccess" 
        subnet_ids         = module.vpc.private_subnets
#       additional_security_group_ids = aws_security_group.lb_sg.id
#      create_node_security_group = true

    }

  # }
  # #Create security group
  # create_node_security_group= {
  #   lb_security = {
  #     ingress = {
  #   from_port   = 80
  #   to_port     = 80
  #   protocol    = "tcp"
  #   cidr_blocks = ["0.0.0.0/0"]
  #     }
  #      egress = {
  #   from_port   = 0
  #   to_port     = 0
  #   protocol    = "-1"
  #   cidr_blocks = ["0.0.0.0/0"]
  # }
  #   }
  }

  # Cluster access entry
  # To add the current caller identity as an administrator
  enable_cluster_creator_admin_permissions = true
  tags = {
    Environment = "dev"
    Terraform   = "true"
  }


  authentication_mode = "API_AND_CONFIG_MAP"
}

output "node_group_id" {
  value = module.eks.eks_managed_node_groups["terra-eks"].node_group_id
}


output "kubeconfig" {
  value={
    cluster_name = module.eks.cluster_name
    endpoint     = module.eks.cluster_endpoint
}
}

# Output the cluster endpoint
output "cluster_endpoint" {
  value = module.eks.cluster_endpoint
}


# Output the cluster kubeconfig command
output "kubeconfig_command" {
  value = "aws eks update-kubeconfig --name ${module.eks.cluster_name} --region us-east-1"
}


# # Launch 3 EC2 Instances
# resource "aws_instance" "computers" {
#   count         = 3
#   ami           = "ami-0453ec754f44f9a4a" # Amazon Linux 2023 AMI 2023.6.20241121.0 x86_64 HVM kernel-6.1
#   instance_type = "t3.micro"
#   subnet_id     = module.vpc.private_subnets

#   tags = {
#     Name = "computer-instance-${count.index + 1}"
#   }
# }



# Security Group for Load Balancer
resource "aws_security_group" "lb_sg" {
  name_prefix = "lb-sg"
  vpc_id      = module.vpc.vpc_id
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "eks_nodes_sg" {
  name_prefix = "eks-nodes-sg"
  vpc_id      = module.vpc.vpc_id
ingress {
  from_port   = 443
  to_port     = 443
  protocol    = "tcp"
  security_groups = [module.eks.cluster_primary_security_group_id]  # Allow control plane traffic
}

ingress {
  from_port   = 10250
  to_port     = 10250
  protocol    = "tcp"
  cidr_blocks = ["10.10.0.0/16"]  # Replace with your VPC CIDR block
}

ingress {
  from_port   = 1025
  to_port     = 65535
  protocol    = "tcp"
  cidr_blocks = ["10.10.0.0/16"]  # Replace with your VPC CIDR block
}

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    security_groups = [aws_security_group.lb_sg.id]  # Allow traffic from LB only
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}


# Application Load Balancer
resource "aws_lb" "lb_cluster" {
  name               = "terra-cluster-lb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = module.vpc.private_subnets
}

# Target Group for Instances
resource "aws_lb_target_group" "cluster-group" {
  name     = "cluster-target-group"
  port     = 80
  protocol = "HTTP"
  target_type = "instance"
  vpc_id   = module.vpc.vpc_id
}

  # Attach Instances to Target Group
  resource "aws_lb_target_group_attachment" "lb_group_attach" {
    count            = 3
    target_group_arn = aws_lb_target_group.cluster-group.arn
    target_id = module.eks.eks_managed_node_groups["terra-eks"].node_group_id
    port             = 80
  }
# Listener
resource "aws_lb_listener" "listen_lb" {
  load_balancer_arn = aws_lb.lb_cluster.arn
  port              = 80
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.cluster-group.arn
  }
}



resource "aws_iam_role_policy_attachment" "example-AmazonEKSWorkerNodePolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = module.eks.cluster_iam_role_name
}

resource "aws_iam_role_policy_attachment" "example-AmazonEKS_CNI_Policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = module.eks.cluster_iam_role_name
}

resource "aws_iam_role_policy_attachment" "example-AmazonEC2ContainerRegistryReadOnly" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = module.eks.cluster_iam_role_name
}



resource "aws_iam_group_policy_attachment" "technition_policy_for_cluster" {
  group = "tech_group"
 policy_arn ="arn:aws:iam::aws:policy/AdministratorAccess"
}



# resource "aws_iam_role" "eks_role-admin" {
#   name = "eks-role-admin"

#   assume_role_policy = jsonencode({
#     Version = "2012-10-17",
#     Statement = [
#       {
#         Effect    = "Allow",
#         Principal = {
#           Service = "eks.amazonaws.com"
#         },
#         Action    = "sts:AssumeRole"
#       }
#     ]
#   })
# }

# # Attach group policy
# resource "aws_iam_role_policy_attachment" "group_policy" {
#   role       = aws_iam_role.eks_role-admin.name
#   policy_arn = "arn:aws:iam::aws:policy/AdministratorAccess"  # Example policy
# }


# resource "aws_iam_instance_profile" "ec2_instance_profile" {
#   name = "eks-instance-profile-admin"
#   role = aws_iam_role.eks_role-admin.name
# }

resource "local_sensitive_file" "EKS_ip" {
  filename = "${path.module}/variables.yaml"

  content = <<EOT
---
flask-app-eks:
%{ for id in flatten(module.eks.eks_managed_node_groups["terra-eks"].node_group_id) }
  - "${id}"
%{ endfor }
EOT

  depends_on = [module.eks]
}



resource "local_sensitive_file" "EKS_inv" {
  filename   = "/home/vagrant/Documents/Devops/Major_Project/MajorProject/terraform/tf_ec2_ansible/inventory.ini"
  content    = <<EOT
[web_servers]
%{ for id in flatten(module.eks.eks_managed_node_groups["terra-eks"].node_group_id) }
 "${id}" ansible_ssh_user=ubuntu ansible_ssh_private_key_file=/home/vagrant/.aws/My-first-key.pem
%{ endfor }

EOT
  depends_on = [module.eks]
}


  resource "local_sensitive_file" "my_eip_ip" {
   content  = <<EOT
   %{ for ip in flatten(module.eks.eks_managed_node_groups["terra-eks"].node_group_id) }
  - "${ip}"instance[*].id
%{ endfor }
EOT
   filename = "${path.module}/my_eip_ip.txt"
   depends_on = [ module.eks ]
}